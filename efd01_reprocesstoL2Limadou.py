#!/usr/bin/env python3
"""
This script reprocesses EFD-01 L2 waveforms from the ULF and ELF bands in order to fix the following
errors present in the public EFD-01 L2 data generated by

1) errors in the rotation of Electric field to the WGS84 reference frame
2) discretization errors in the timing.
3) presence of gaps in the data. These gaps are substituted with nans

calling sequence

./python3 -input <efd_filename.h5> [-output output_filename.h5]  

input arguments

    <efd_filename.h5>: filename containing the desired data to process

    output_filename.h5: output file (optional). If not given the filename
        <efd_filename_L2B.h5> is used.

"""
"""
Messenger package to stdout 
"""
from termcolor import colored
import os
import stat
VIOLET = '\033[95m'
BLUE = '\033[94m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
RED = '\033[91m'
BOLD = '\033[1m'
UNDERLINE = '\033[4m'
END = '\033[0m'


def _print(text, indent, color):

    """Print text in given color, preceded by given #tabs.

    Keyword arguments:
    text -- text to print
    indent -- no. of tabs
    color -- text color defined by global variables
    """

    if indent:
        print('\t' * indent,)
    print(color + text + END)


def msginfo(text, indent=None):

    """Precede text with 'INFO' and call _print with GREEN color."""

    _print("\nINFO: " + text, indent, GREEN)


def warning(text, indent=None):

    """Precede text with 'WARNING' and call _print with YELLOW color."""

    _print("\nWARNING: " + text, indent, YELLOW)


def error(text, indent=None):

    """Precede text with 'ERROR' and call _print with RED color."""

    _print("\nERROR: " + text, indent, RED)

def news(text, indent=None):

    """Precede text with 'ERROR' and call _print with RED color."""

    _print("\nNEWS: " + text, indent, VIOLET)

def box(text, width=80):

    """'Draw box' and print text in the center (using BOLD font)."""

    print(BLUE)
    pad = (width - len(text)) // 2
    print('+' + '-' * width + '+')
    print('|' + ' ' * width + '|')
    print('|' + ' ' * pad + text + ' ' * (width - pad - len(text)) + '|')
    print('|' + ' ' * width + '|')
    print('+' + '-' * width + '+')
    print(END)

def INFO(text):
    return colored(text,'green')
def WARNING(text):
    return colored(text,'yellow')
def ERROR(text):
    return colored(text,'red',attrs=['bold'])


import numpy as np
import os
from datetime import datetime,timedelta,date
from glob import glob


CSES_DATA_TABLE = {'EFD':{'1':'ULF','2':'ELF','3':'VLF','4':'HF'}}

CSES_FILE_TABLE = {'EFD':{\
                       '1':{'A111_W':'Ex',\
                            'A112_W':'Ey',\
                            'A113_W':'Ez'
                           },\
                       '2':{'A121_W':'Ex',\
                            'A122_W':'Ey',\
                            'A123_W':'Ez'
                           },\
                       '3':{'A131_W':'Ex',\
                            'A132_W':'Ey',\
                            'A133_W':'Ez'
                           },\
                         }}
CSES_POSITION = {'ALTITUDE':'alt',\
                 'GEO_LAT':'lat',\
                 'GEO_LON':'lon',\
                 'MAG_LAT':'mag_lat',\
                 'MAG_LON':'mag_lon'}

CSES_DATASETS = {'A111_P':'Ex_P','A111_W':'Ex',\
                 'A112_P':'Ey_P','A112_W':'Ey',\
                 'A113_P':'Ez_P','A113_W':'Ez',\
                 'A121_P':'Ex_P','A121_W':'Ex',\
                 'A122_P':'Ey_P','A122_W':'Ey',\
                 'A123_P':'Ez_P','A123_W':'Ez',\
                 'A131_P':'Ex_P','A131_W':'Ex',\
                 'A132_P':'Ey_P','A132_W':'Ey',\
                 'A133_P':'Ez_P','A133_W':'Ez'}

CSES_SAMPLINGFREQS = {'EFD_ULF':125.,'EFD_ELF':5000.}
CSES_PACKETSIZE = {'EFD_ULF':256,'EFD_ELF':2048}

################################################################################
####################           AUXILIARY FUNCTIONS           ###################
################################################################################
def parse_CSES_filename(filename):

    fl_list = filename.split('_')
    out={}
    if len(filename) == 66:
        out['Satellite'] = fl_list[0]+fl_list[1]
        out['Instrument'] = fl_list[2]
        try:
            out['DataProduct'] = CSES_DATA_TABLE[fl_list[2]][fl_list[3]]
        except:
            out['DataProduct'] = 'Unknown' 
        out['InstrumentNum'] = fl_list[3]
        out['DataLevel'] = fl_list[4]
        out['orbitn'] = fl_list[6]
        out['year'] = fl_list[7][0:4]
        out['month'] = fl_list[7][4:6]
        out['day'] = fl_list[7][6:8]
        out['time'] = fl_list[8][0:2]+':'+fl_list[8][2:4]+':'+fl_list[8][4:6]
        out['t_start'] = datetime(int( out['year']),int(out['month']),int(out['day']),\
                            int(fl_list[8][0:2]),int(fl_list[8][2:4]),int(fl_list[8][4:6])) 
        out['t_end'] = datetime(int(fl_list[9][0:4]),int(fl_list[9][4:6]),int(fl_list[9][6:8]),\
                            int(fl_list[10][0:2]),int(fl_list[10][2:4]),int(fl_list[10][4:6]))
    elif len(filename) == 69:
        out['Satellite'] = fl_list[0]+'_01'
        out['Instrument'] = fl_list[1]
        out['DataProduct'] = fl_list[2]
        out['DataLevel'] = fl_list[-2]
        out['orbitn'] = fl_list[3]
        out['year'] = fl_list[4][0:4]
        out['month'] = fl_list[4][4:6]
        out['day'] = fl_list[4][6:8]
        out['time'] = fl_list[5][0:2]+':'+fl_list[5][2:4]+':'+fl_list[5][4:6]
        out['t_start'] = datetime(int( out['year']),int(out['month']),int(out['day']),\
                            int(fl_list[5][0:2]),int(fl_list[5][2:4]),int(fl_list[5][4:6])) 
        out['t_end'] = datetime(int(fl_list[6][0:4]),int(fl_list[6][4:6]),int(fl_list[6][6:8]),\
                            int(fl_list[7][0:2]),int(fl_list[7][2:4]),int(fl_list[7][4:6]))

    return out


def fix_lonlat(lons,lats,times):
    from scipy.interpolate import splrep,splev
    time = times.flatten()
    dt = np.diff(time)
    lon = lons.flatten()
    lat = lats.flatten()
    mm = np.zeros(len(lat),dtype=bool)
    if np.median(np.diff(lat)) < 0:
        mm[1:-1] = ~((lat[1:-1]>lat[2::])*(lat[1:-1]<lat[0:-2]))
    else:
        mm[1:-1] = ~((lat[1:-1]<lat[2::])*(lat[1:-1]>lat[0:-2]))
    if np.sum(mm) > 0:
       tck = splrep(time[~mm],lat[~mm])
       lat[mm] = splev(time[mm],tck)

    mm = np.zeros(len(lon),dtype=bool)
    dalon = np.abs(np.diff(lon))
    mjumps = (dalon>350)*(dalon<360)
    if np.median(np.diff(lon)) < 0:
        mm[1:-1] = ~((lon[1:-1]>lon[2::])*(lon[1:-1]<lon[0:-2]))
    else:
        mm[1:-1] = ~((lon[1:-1]<lon[2::])*(lon[1:-1]>lon[0:-2]))
    mm[1:][mjumps] = False
    if np.sum(mm) > 0:
       tck = splrep(time[~mm],lon[~mm])
       lon[mm] = splev(time[mm],tck)
    return lon.reshape(lons.shape),lat.reshape(lats.shape)

def datenum(yy,mm,dd, utc = None):
    """
    Convert string to datetime objects
    Assumes utc is a string of format YYYYMMDDHHMMSSms
    """
    
    if utc is None:
        return datetime(yy,mm,dd)
    
    if len(utc) > 14 :
        utcdate = datetime(int(utc[0:4]),int(utc[4:6]),int(utc[6:8]),\
                int(utc[8:10]),int(utc[10:12]),int(utc[12:14]),int(utc[14:])*1000)
    else:
        utcdate = datetime(int(utc[0:4]),int(utc[4:6]),int(utc[6:8]),\
                int(utc[8:10]),int(utc[10:12]),int(utc[12:14]))

    return datetime(yy,mm,dd), utcdate

def add_packets(xbig,jumps,npacks,dt,fill_missing = 'sampling'):
    """
    fills gap according to fill_missing option
    xbig : 2D array of size nrows,packet_size
    jumps: 1D array of indices where you have jumps (of size NJUMPS)
    npacks: 1D integer array of size NJUMPS containing the number of missing packets
    dt : sampling time (if fill_missing == 'sampling')
    """
    nrows,packet_size = xbig.shape
    xout = xbig.copy()
    
    mask = np.ones(nrows,dtype=bool)
    #filling missing columns starting from the end
    for i,nx in zip(np.flipud(jumps),np.flipud(npacks)): 
        
        #inserting packets
        xout = np.insert(xout,[i+1]*nx,np.zeros(packet_size),axis=0)
        mask = np.insert(mask,[i+1]*nx,False)
        #xout = np.insert(xout, jumps+1,np.zeros(packet_size),axis=0)  
        if fill_missing == 'sampling':
            xout[i+1:i+nx+1] = (np.arange(nx*packet_size).reshape((nx,packet_size))+packet_size)*dt + xbig[i,0]

    return xout, mask

def interp1(x,xp,fp,fill_value='extrapolate',**kwargs):
    
    from scipy.interpolate import interp1d 
    finterp=interp1d(xp,fp,fill_value=fill_value,**kwargs)
    return finterp(x)

def interp1_jumps(xint,x,y,interval):
    """
    interpolate y defined on x to y defined on xint
    if y is a function defined in an domain with interval [a,b] (e.g. from -180 to 180)
    and x is such that y is discontinous (e.g. arcsin), the function removes the jumps 
    before interpolating and restores them afterwards.
    """

    #from numpy import interp as interp1


    yper = y.flatten()

    dy = np.diff(yper)

    intdif = interval[1]-interval[0]

    dymed = np.median(np.abs(dy))
    mask = (dy>dymed)* (dy>intdif*0.51)
    #print(np.sum(mask))
    if np.sum(mask) >0:
        for i in np.where(mask)[0]:
            yper[i+1:] -= np.sign(dy[i])*intdif
            
        return np.mod(interp1(xint,x,yper) - interval[0],intdif) + interval[0]
    else:
        return interp1(xint,x,yper)

def dict_to_recarray(dic):
    """
    Convert a list of structurally identical dictionaries (i.e. dictionaries with same keys, 
    each of which being a numpy.ndarray of same shape for all keys and for all the dictionaries 
    inside the list) to a numpy.recarray
    """
    keys = [i for i in dic.keys()]
    if hasattr(dic[keys[0]],'shape'):
        nx = dic[keys[0]].shape 
        dtyps= [type(dic[i].flatten()[0]) for i in keys]
    else:    
        nx = np.size(dic[keys[0]])
        dtyps= [type(np.asarray(dic[i]).flatten()[0]) for i in keys]
    
    out = np.recarray(nx,dtype = [(i,dtyp) for i,dtyp in zip(keys,dtyps)])
    
    for i in out.dtype.names: out[i] =  dic[i]

    return out

def get_rotation_matrix_from_vectors(vec1, vec2):
    """ Find the rotation matrix that aligns vec1 to vec2
    :param vec1: A 3d "source" vector
    :param vec2: A 3d "destination" vector
    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.
    """
    a, b = (vec1 / np.linalg.norm(vec1)).reshape(3), (vec2 / np.linalg.norm(vec2)).reshape(3)
    v = np.cross(a, b)
    if any(v): #if not all zeros then 
        c = np.dot(a, b)
        s = np.linalg.norm(v)
        kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
        return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))

    else:
        return np.eye(3) #cross of all zeros only occurs on identical directions

def find_rotational_jumps(EE,keys,nskip, n_sigma = 4.,mask = None):
    """
    finds rotational jumps location in the electric field, using outlier detection.
    
    Procedure:
        1) Calculates candidates jump values of EE at the packets edges.
                spks = Ex[i*nskip] -  Ex[i*nskip-1]
        2) compute the median of the differences in an interval close to the edge.
        3) compute std of such differences
        4) if spks/std > n_sigma, then the candidate jump is identified as an outlier hence is a jump. 
    """

    dn = np.max([15,int(nskip//50)])
    nn = np.size(EE[keys[0]])
    kjump = []
    for i in range(nskip,nn,nskip):
        
        if mask is not None:
           if mask[i]: continue 
        spks = [np.abs(EE[ikey][i] - EE[ikey][i-1]) for ikey in keys]
        x0 = [np.nanmedian(np.diff(EE[ikey][i-dn:i+dn])) for ikey in keys]
        stds = [np.nanmedian(np.abs(np.diff(EE[ikey][i-dn:i+dn]) - ix0)) for ix0,ikey in zip(x0,keys)]
        
        if any([ispk/istd > n_sigma for ispk,istd in zip(spks,stds)]):
            kjump.append(i)

    return kjump
################################################################################
####################              MAIN FUNCTIONS             ###################
################################################################################

def CSES_load(filename,path, return_pandas = False,
            with_mag_coords = False,keep_verse_time = True, fill_missing='linear'):
    """
    Generic method to read EFD CSES DataProduct
    Filename conventions should be according to the following example:
        
        'CSES_01_EFD_2_L02_A1_031190_20180826_095004_20180826_102510_000.h5'

    data are loaded into a pandas dataframe. Time is taken by the VERSE_TIME variable
    present in the h5 file, but sampling rate is used to calculate dt, due to truncation
    error introduced in the data (it is an integer number in milliseconds,
    but dt sampling may not be a multiple integer of 1ms)

    Parameters
    ----------
    filename : str
            string containing the name of the file
    path : str 
        string containing the filepath.
    return_pandas : bool
        if True, data are returned as a pandas dataframe.
    with_mag_coords : bool
        if True, magnetic coordinates contained in the file are also loaded
        (to avoid since mag coords contained in the data are often wrong).
    keep_verse_time : bool
        if True, add the VerseTime to the output.
    fill_missing : str or None or np.nan or float
        Determines filling method for gaps in the data. 
        If set to float, fill gaps with desired value.
        Allowed values:
          
          None : it does not fill any temporal gap/missing packets
          
          'zero'or 0 : fills gaps with zeroes.
          
          'nan' or np.nan : fills gaps with NaNs
          
          float : fills gaps with the desired floating value
          
          'linear': fits the gaps with a linear function between the two points
          

    Output: (res, aux) (tuple)
    ------
        res : numpy.recarray or pandas.dataframe 
            contains instrument data and coordinate data
        aux : dict
            contains ancillary data with the following keywords:
            {'ORBITNUM':int,
             'units':'V/m', (for the electric field, nT for B, etc.)
             'UTC':utc time of first datapoint, 
             'verse_time': VERSE time of first datapoint,
             'verse_zero_utc': utc time of the zero VERSE time (i.e, 2009/1/1) }
    """
    
    import h5py
    import pandas as pd
    from scipy.interpolate import interp1d 
   


    info = parse_CSES_filename(filename)
    fldtags = CSES_FILE_TABLE[info['Instrument']][info['InstrumentNum']]
 
    fil = h5py.File(path+filename,'r')
    orbitnum = int(fil.attrs['ORBITNUM'][0])
    
    try:
        units = {fldtags[i]:fil[i].attrs['units'][0] for i in fldtags}
    except:
        try:
            units = {fldtags[i]:[fil[i].attrs[j][0] for j in fil[i].attrs.keys()][0] for i in fldtags}
        except:
            fldtags = CSES_DATASETS
            try:
                units = {fldtags[i]:fil[i].attrs['units'][0] for i in fldtags if i in fil}
            except:
                units = {fldtags[i]:[fil[i].attrs[j][0] for j in fil[i].attrs.keys()][0] \
                            for i in fldtags if i in fil}
            fldtags = {i:fldtags[i] for i in fldtags if i in fil}
    data =  {fldtags[i]:fil[i][...] for i in fldtags}
    pos = {CSES_POSITION[i]:fil[i][...] for i in CSES_POSITION}

    ms, ns = dshape = data[[fldtags[i] for i in fldtags][0]].shape


    if not with_mag_coords:
        if 'mag_lat' in pos : del pos['mag_lat']
        if 'mag_lon' in pos : del pos['mag_lon']
    
    Vtime = fil['VERSE_TIME'][...]
    Utime = fil['UTC_TIME'][...]
    
    #loading spectra
    psddata =  {CSES_DATASETS[i]:fil[i][...] for i in CSES_DATASETS if i in fil and i[-2:]=='_P'}
    fil.close()
    
    #fixing bad jumps in orbital position
    lont,latt = fix_lonlat(pos['lon'],pos['lat'],Vtime)
    pos['lon'] = lont; pos['lat'] = latt
    del lont,latt
    
    #convert from CSES date (VERSE_TIME) to standard date
    vt0_utc, utc = datenum(2009,1,1,utc = str(Utime[0][0]))    #CSES initial time
    Utime = np.array([j[1] for j in [datenum(2009,1,1,utc=str(i[0])) for i in Utime]])

    tx=Vtime
    Vtime0 = tx[0][0] #VERSE_TIME a t=0 in milliseconds
    tx -=Vtime0
    tx = tx/1000
    Vtime0/=1000
    time1=tx     #verse_time in seconds
    del tx

    packet_size  = ns
    dtrate = 1/CSES_SAMPLINGFREQS[info['Instrument']+'_'+info['DataProduct']]
    do_interp = dshape != time1.shape

    time1 = time1.flatten()
    time = np.zeros(dshape)
    
    #finding missing packages positions (time jumps)
    jumps = np.where(np.diff(time1)>packet_size*dtrate*1.2)[0]
  
    #filling the time array
    ij0=0
    for ij in jumps:
        time[ij0:ij+1,:] = time1[ij0] +dtrate*np.arange((ij+1-ij0)*ns).reshape((ij+1-ij0,ns))
        ij0 = ij+1
    #last chunk
    if ij0<ms: 
        time[ij0:,:] = time1[ij0] +dtrate*np.arange((ms-ij0)*ns).reshape((ms-ij0,ns))

    t_old = time1.copy()
    if fill_missing is not None:
        #calculating gap in terms of number of packets missing
        dtjumps = np.diff(time1)[jumps] 
        npacks = np.rint(dtjumps/(packet_size*dtrate)).astype(int)-1
        #filling with linear interpolation
        t_new, mask_old = add_packets(time,jumps,npacks,dtrate)
    else:
        t_new = time.copy()
        mask_old = np.ones(ms,dtype = bool)

    msnew, ns = t_new.shape
    t_new = t_new.flatten()
    t_new =np.round(t_new*1e6)/1e6
    #because Vtime has a precision of 1ms (i.e. 1kHz) and the sampling rate is 5kHz,
    #it happens that some delta_t is negative between the packets.
    #a quick fix is to simply shift by 5kHz the whole time array in those points
    neg_t = np.where(np.diff(t_new)<0)[0]
    if np.size(neg_t)>0:
        for it in neg_t:
            dt = t_new[it+1] -t_new[it]
            t_new[it+1:] += -dt + dtrate
    
    t_old = t_new.reshape((msnew,ns))[mask_old,0]
    t_new = t_new.flatten()

    if do_interp:
        #interpolate various positions
        pos1 = {i:interp1(t_new,time1.flatten(),pos[i].flatten()) for i in pos}
        #interpolate longitude 
        if 'lon' in pos:
            pos1['lon'] = interp1_jumps(t_new,time1.flatten(),pos['lon'].flatten(),np.array([-180,180]))
        if 'mag_lon' in pos:
            pos1['mag_lon'] = interp1_jumps(t_new,time1.flatten(),\
                pos['mag_lon'].flatten(),np.array([-180,180]))
        del pos
        pos = pos1
    else:
        pos1 = {i:pos[i].flatten() for i in pos}

    if fill_missing is not None and any(~mask_old):
        
        data1 = {i:np.zeros((msnew,ns)) for i in data}
        for i in data: data1[i][mask_old] = data[i]

        if type(fill_missing) is not str:
            for i in data.keys(): data1[i][~mask_old] = fill_missing

        elif fill_missing == 'zero':
            for i in data.keys(): data1[i][~mask_old] = 0

        elif fill_missing == 'nan':
            for i in data.keys(): data1[i][~mask_old] = np.nan

        elif fill_missing == 'linear':
            data1 = {i:interp1(t_new,t_new.reshape((msnew,ns))[mask_old].flatten(),data[i].flatten()) for i in data}

        data = data1 
        mm = np.zeros((msnew,ns),dtype = bool)
        for i,j in enumerate(mask_old): mm[i]=j
        data['gaps_mask'] = ~mm.flatten()
   
        if any([pos1[i].shape != data['gaps_mask'].shape for i in pos1]):
            pos1 = {i:interp1(t_new,t_new.reshape((msnew,ns))[mask_old].flatten(),pos1[i].flatten()) for i in pos1}
    
    if fill_missing is not None and 'gaps_mask' not in data:
        data['gaps_mask'] = np.zeros(data[[i for i in data][0]].shape,dtype=bool)

    if fill_missing is not None:
        for ipsd in psddata:
            dum = np.zeros((msnew,psddata[ipsd].shape[-1]))
            dum[mask_old] = psddata[ipsd]
            dum[~mask_old] = np.nan
            psddata[ipsd] = dum
            del dum

    data = {i:data[i].flatten() for i in data} 
    data.update(pos1)
    data['time'] = t_new.flatten()
    
    if info['Instrument'] == 'EFD':
        for i in fldtags:
            if units[fldtags[i]] == b'mV/m': 
                data[fldtags[i]] /=1000
                units[fldtags[i]] = b'V/m'

    res = dict_to_recarray(data)
    index = pd.to_timedelta( res['time'] - res['time'][0],unit='sec') + utc
    if return_pandas:
        df = pd.DataFrame(res,index=index)
        if not keep_verse_time : 
            df.drop('time',axis='columns',inplace=True)
        else:
            df['time']+=Vtime0
        df['orbitn']=int(info['orbitn'])
        res = df 


    return res,psddata, {'ORBITNUM':orbitnum,'units':units ,'UTC':utc, 'verse_zero_utc':vt0_utc, 'verse_time':utc-vt0_utc,'utc_time':index}



def derotate_fields(df,frequency, nskip_fixed = False):
    """
    Remove the artificial jumps between data packets introduced by rotations performed 
    in the chinese CSES01 EFD pipeline to rotate the electric field from the spacecraft 
    frame to ECEF. The jumps are present every nskip steps or a multiple of that number
    (length of one packet). 
    
    Optional Parameters
    -------------------
    nskip_fixed: bool
        if not fixed, the algorithm will look whether jumps are present or not in the data, 
        at multiples of data packet size (2048 for EFD_ELF) and if so, will update 
        the rotation between two jumps.
        This workaround is necessary because for some orbits of CSES, for the EFD ELF data 
        was found that this number (which is usually 2048) appears to be 4096 instead.
        Must be checked with the L2 data processing by the chinese side.

    
    It compensate for the wrong rotation in the following way:
        1) It assumes that the electric field vectors measured at every i*nskip 
           (i.e. at the beginning of the packet) are correctly rotated,
        2) calculates the rotation matrix from i*nskip-1 to i*nskip
        3) create a sequence of matrices which perform a rotation from the first point 
           of the packet (i.e. i*nskip-nskip) to the last point (i*nskip-1) calculated by 
           interpolating from zero rotation (identity matrix) to the calculated rotation matrix.
        4) Performs the rotation of the points.

    N.B. The orientation of the electric field with respect to (e.g.) GCS reference frame 
         should be preserved in this way.

    
    """

    nskip = CSES_PACKETSIZE['EFD_'+frequency] 
    print('Derotating fields...')
    #1-removing jumps by derotating artificially
    Ex,Ey,Ez = df['Ex'],df['Ey'],df['Ez']
    mask = df['gaps_mask']

    nn =  np.size(Ex)
    oex = np.copy(Ex)
    oey = np.copy(Ey)
    oez = np.copy(Ez)
    EE =  np.array([Ex,Ey,Ez])
    
    def perform_rotation(ex,ey,ez,mrot):
        from scipy.spatial.transform import Rotation as R
        from scipy.spatial.transform import Slerp
        #creating interpolant
        nskip = np.size(ex)
        mrotl2r = np.zeros((2,3,3)) 
        mrotl2r[0] = np.eye(3) #identity matrix
        mrotl2r[1] = mrot
        slerpint = Slerp([0,1],R.from_matrix(mrotl2r))
        #rotation matrices from identity to mrot
        interp_rot = slerpint(np.arange(nskip)/nskip).as_matrix()
        
        BB = np.array([ex,ey,ez]).transpose()
        return np.array([np.dot(interp_rot[i],BB[i]) for i in range(nskip)]).transpose()
        
    if nskip_fixed:
        mats = []
        for i in range(nskip,nn,nskip):
        
            il = i-1 
            ir = i
        
            r=np.array((oex[ir],oey[ir],oez[ir]))
            l=np.array((oex[il],oey[il],oez[il]))
        
            l/= np.sqrt(np.sum(l**2))  
            r/= np.sqrt(np.sum(r**2))  
            mat = get_rotation_matrix_from_vectors(l,r) #rotate from l to r
            mats.append(mat)
            
            ee = perform_rotation(Ex[i-nskip:i],Ey[i-nskip:i],Ez[i-nskip:i],mat)
       
            EE[:,i-nskip:i] = ee 
    else:
        mats = []
        jumps = find_rotational_jumps({'Ex':Ex,'Ey':Ey,'Ez':Ez},['Ex','Ey','Ez'],nskip,mask = mask)
        for j,i in enumerate(jumps):
        
            il = i-1 
            ir = i
        
            r=np.array((oex[ir],oey[ir],oez[ir]))
            l=np.array((oex[il],oey[il],oez[il]))
        
            l/= np.sqrt(np.sum(l**2))  
            r/= np.sqrt(np.sum(r**2))  
            mat = get_rotation_matrix_from_vectors(l,r) #rotate from l to r
            mats.append(mat)
            
            #NOW rotating electric field in the packet
            if j == 0:
                ee = perform_rotation(Ex[i-nskip:i],Ey[i-nskip:i],Ez[i-nskip:i],mat)
                EE[:,i-nskip:i] = ee 
            else:
                ee = perform_rotation(Ex[jumps[j-1]:i],Ey[jumps[j-1]:i],Ez[jumps[j-1]:i],mat)
                EE[:,jumps[j-1]:i] = ee 

    df['Ex'] = EE[0] 
    df['Ey'] = EE[1] 
    df['Ez'] = EE[2] 



def fill_missing_packets(df,strfill):
    """
    fill missing packets with desired value
    """
    print(strfill)
    if strfill == 'None': 
        df = df[~df['gaps_mask']]
    if strfill == 'nan' :
        for i in ['Ex','Ey','Ez']: df[i][df['gaps_mask']] = np.nan
    if strfill == 'zero' :
        for i in ['Ex','Ey','Ez']: df[i][df['gaps_mask']] = 0

    return df


def convert_geo2qd(lat,lon,alt,time):
    """
    Convert geographic coordinates to QuasiDipole coordinates.
    Parameters
    lat : np.array
        Array of latitudes in geodetic coordinates.
    lon : np.array
        Array of longitudes in geodetic coordinates.
    alt : np.array
        Array of altitudes in geodetic coordinates.
        Datetime object to select IGRF coefficients.
    **kwargs : dict, optional
        Additional keyword arguments.
    Returns
    -------
    tuple
        A tuple containing QuasiDipole coordinates (qd_lat, qd_lon, qd_alt).
    Notes
    -----
    This function uses the `apexpy` library to perform the conversion.

    geographic-to-QuasiDipole coordinates conversion 
    
    """
    
    #dates = efd.index.to_pydatetime()
    #import aacgmv2
    #aacgm = [aacgmv2.get_aacgm_coord(efd.lat[i],efd.lon[i],efd.alt[i],dates[i]) for i in range(efd.shape[0])]    
    
    
    import apexpy 
    # Recast the data as numpy arrays
    
    apx = apexpy.Apex(date=time)

    return apx.geo2qd(lat,lon,alt)

def add_column_to_recarray(recarray, new_column):
    """
    Add a new column to a numpy recarray.

    Parameters:
        recarray (np.recarray): The original record array.
        new_column (tuple): A tuple of (field_name, data_array).

    Returns:
        np.recarray: A new recarray with the additional column.
    """
    name, values = new_column

    if len(values) != len(recarray):
        raise ValueError("Length of new column does not match recarray")

    # Determine dtype of the new column from the array
    new_dtype = np.dtype(recarray.dtype.descr + [(name, values.dtype)])

    # Create new recarray
    new_rec = np.recarray(recarray.shape, dtype=new_dtype)

    # Copy existing data
    for field in recarray.dtype.names:
        new_rec[field] = recarray[field]

    # Add new column data
    new_rec[name] = values

    return new_rec

def add_QD_coords(df):
    
    mlat,mlon = convert_geo2qd(df.lat,df.lon,df.alt,df.index[0].to_pydatetime())
    df['MAG_LAT'] = mlat
    df['MAG_LON'] = mlon
    #df = add_column_to_recarray(df,('Mlon',mlon))
    #df = add_column_to_recarray(df,('Mlat',mlat))
################################################################################
####################              OUTPUT H5 FUNCTIONS        ###################
################################################################################
def save_human_readable(df,psds,aux,outfile):

    import h5py
    fil = h5py.File(outfile,'w')
    idx={'VERSETIME0':aux['verse_time'].total_seconds(),'UTC_TIME0':str(aux['utc_time'][0])}
    for i in df.dtype.names:
        if type(df[i][0]) == np.bool_:
            fil.create_dataset(i,data=df[i].astype(int))
        elif i == 'time':
            fil.create_dataset(i,data=np.round((df['time']*1000000)).astype(int))
        
        else:
            fil.create_dataset(i,data=df[i])
    vtime0 = int(aux['verse_time'].total_seconds()*1e6)
    fil['time0'] = vtime0
    fil['time0'].attrs['units'] = 'microseconds'
    for i in psds:
        npsd = psds[i].shape[0]
        fil.create_dataset(i,data=psds[i])
    nskip = int(df.shape[0]/npsd)
    [fil.create_dataset(i+'_P',data=df[i][::nskip]) \
        for i in ['alt','lat','lon','gaps_mask']]
    fil.create_dataset('time_P',data=np.round(df['time'][::nskip]*1000000).astype(int))
    for i in idx:
        fil.create_dataset(i,data=idx[i])
        #print("unable to save dataframe to file, please find the mistake!")
    fil.close()

def save_original_format(df,aux,info,infile,outfile,fill_missing):
    
    import shutil
    import h5py
    
    #os.remove(outfile)
    shutil.copy(infile,outfile)
    os.chmod(outfile,stat.S_IWUSR | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)
    fil = h5py.File(outfile,'r+')
    
    fldtags = CSES_FILE_TABLE[info['Instrument']][info['InstrumentNum']]
    datakey = {i[1]:i[0]  for i in fldtags.items()}
    
    ms = fil[datakey['Ex']].shape[-1]
    ns = int(df.shape[0]/CSES_PACKETSIZE[info['Instrument']+'_'+info['DataProduct']])
    mask = df['gaps_mask'].reshape((ns,ms))[:,0]
    if fill_missing != 'None' : 
        fil['gaps_mask'] = mask#.astype(int)
        mask[:] = False
    
    for ikey in ['Ex','Ey','Ez']:
        del fil[datakey[ikey]]
        dum = df[ikey].reshape((ns,ms))[~mask]
        fil[datakey[ikey]] = dum

    poskey = {i[1]:i[0]  for i in CSES_POSITION.items()}
    for ikey in ['lat','lon','alt']:
        del fil[poskey[ikey]]
        dum = df[ikey].reshape((ns,ms))[~mask]
        fil[poskey[ikey]] = dum[:,0]

    #saving time in float
    del fil['VERSE_TIME']
    vtime0 = int(aux['verse_time'].total_seconds()*1e6)
    vtimes = np.round((df['time'].reshape((ns,ms))[~mask][:,0]*1000000)).astype(int)
    fil['DVERSE_TIME'] = vtimes
    fil['VERSE_TIME0'] = vtime0
    fil['VERSE_TIME0'].attrs['units'] = 'microseconds'
    fil['DVERSE_TIME'].attrs['units'] = 'microseconds'

    del fil['UTC_TIME']
    uttime = aux['utc_time'][::ms][~mask]
    uttime = np.array([int(i[:-2]) for i in uttime.strftime('%Y%m%d%H%M%S%f')])
    fil['UTC_TIME'] = uttime
    fil['UTC_TIME'].attrs['format']='YYYYmmddHHMMSSuuuu (u=100microseconds)'
    fil.close()


def parser():
    """parse arguments"""
    import argparse
    nl="\n"
    parser = argparse.ArgumentParser(
        description="EFD-01 L2 -> L2Limadou data processor",
        usage="./efd01_reprocesstoL2Limadou.py <options>",
        formatter_class = argparse.RawTextHelpFormatter,
        epilog="""
Additional Information
  - Output verse time is in microseconds to fix the discretization error due to the use of milliseconds for VERSE_TIME. 
    Output time is saved as 'DVERSE_TIME' (time since first datapoint) and VERSE_TIME0 (time of first data point) in the default format and as 'time' in the human_output format.
        """
        )
    
    #required = parser.add_argument_group("required arguments")

    parser.add_argument(
        "--input", action="store", dest="input",
        metavar="[path/to/filename]", required=True,
        help="path to input hdf5 file"
    )
    
    #optional = parser.add_argument_group("optional arguments")

    parser.add_argument(
        "--output", action="store", dest='output',
        metavar="[path/to/filename] or [filename]", required=False,
        help="output hdf5 file name: can be either the filename or the full path. If only the filename is given then uses the path specified by --output_path (default ./)"
        )

    parser.add_argument("--output_path",action="store",dest='output_path',
        metavar="[path_to_filename/] (optional)", required=False,default='./',
        help="path to output hdf5 file, used if absolute path NOT specified in filename path in --output"
        )

    parser.add_argument("--human_output", help="create output hdf5 file with self-explaining datasets",
                    action="store_true")
    
    parser.add_argument("--add_mag_coords", help="adds geomagnetic coordinates",
                    action="store_true")
    #parser.add_argument("--keep_psd", help="keep PSD datasets in output",
    #                action="store_true")
    parser.add_argument("--fill_missing", help="fill missing data packets with desired value. Default is nan",
                    action="store", default="nan",choices=(['zero','nan','linear','None']))

    parser.add_argument("--constant_nskip", help="if this option is NOT set, then the algorithm looks whether jumps are \
present or not in the data, at multiples of data packet size (2048 for EFD_ELF), \
and if so, the algorithm updates the rotation between two jumps. If set, the algorithm assumes jumps are always present between each packet.",
                    action="store_true")
    
    parser.set_defaults(constant_nskip=False)
    return parser.parse_args()


def efd01_load_L2Limadou(filpath,nskip_fixed = False,human_output = False,fill_missing = 'nan',outfile = None):
    """
    Executes the main processing logic for EFD-01 L2 -> L2Limadou data processor.

    Parameters
    ----------
    filpath : str
        Path to the input file.
    nskip_fixed : bool
        If True, assumes that jumps are located between each data packet (e.g.,
        2048 for ELF, 256 for ULF). If False, the algorithm will look for jumps in the data
        at multiples of the packet size and update the rotation accordingly.    
    human_output : bool
        If True, saves the output in a human-readable format with self-explaining datasets.
    fill_missing : str
        Determines how to fill missing data packets. Options are:
        - 'None': does not fill any temporal gap/missing packets.
        - 'zero': fills gaps with zeroes.
        - 'nan': fills gaps with NaNs.
    outfile : str or None
        If provided, saves the output to the specified file. If None, does not save to  file.
    Returns
    -------
    df : numpy.recarray
        Processed data.
    aux : dict
        Ancillary data.
    info : dict
        Parsed filename information.
    """
    box("EFD-01 L2 -> L2Limadou data processor (Limadou Scienza+ project)")

    filepath = filpath.split('/')
    path = './' if len(filepath) == 1 else '/'.join(filepath[:-1])+'/'
    filename = filepath[-1]

    info = parse_CSES_filename(filename)

    if info['DataProduct'] not in ['ULF','ELF']:
        error('Input filename is not a valid EFD-01 ELF/ULF filename format')
        exit()
    print('loading file: '+INFO(path+filename))

    df, psds, aux = CSES_load(filename, path,
        return_pandas = False,
        keep_verse_time = True, fill_missing='linear')

    derotate_fields(df, info['DataProduct'], nskip_fixed=nskip_fixed)

    if human_output:
        df = fill_missing_packets(df, fill_missing)
        print('formatting output in human readable format')
        if outfile is not None:
            try:
                print('saving output file (human readable): '+INFO(outfile))
            except :
                error('outfile is not a valid string! skipping saving to file')
            if type(outfile) is str : 
                save_human_readable(df, psds, aux, outfile)
            else:
                error('outfile is not a string! skipping saving to file')
    else:
        if fill_missing != 'None':
            df = fill_missing_packets(df, fill_missing)
        if outfile is not None:
            try:
                print('saving output file (original format): '+INFO(outfile))
            except:
                error('outfile is not a valid string! skipping saving to file')
            if type(outfile) is str:
                save_original_format(df, aux, info, path+filename, outfile, fill_missing)
            else:
                error('outfile is not a string! skipping saving to file')
    msginfo('DONE :)')
    return df, aux, info


if __name__ == '__main__':
    
    box("EFD-01 L2 -> L2Limadou data processor (Limadou Scienza+ project)")

    args = parser()
    print(args)

    filepath = args.input.split('/')
    path = './' if len(filepath) == 1 else '/'.join(filepath[:-1])+'/'
    filename = filepath[-1]

    outfile = '.'.join(filename.split('.')[:-1]) if args.output is None else args.output
    if len(outfile.split('/')) == 1:
        outfile = args.output_path+outfile+'_Limadou.h5'

    info = parse_CSES_filename(filename)
   
    if info['DataProduct'] not in ['ULF','ELF']: 
        error('Input filename is not a valid EFD-01 ELF/ULF filename format')
        exit()
    print('loading file: '+INFO(path+filename))

    #loading data (only timeseries) 
    # here also the problem related to dt is fixed
    # N.B. Linear interpolation must be used otherwise it throws an error
    df, psds, aux = CSES_load(filename, path,\
        return_pandas = False,\
        keep_verse_time = True,fill_missing='linear',with_mag_coords = args.add_mag_coords)

    #derotating data
    derotate_fields(df,info['DataProduct'],nskip_fixed=args.constant_nskip)

    if args.add_mag_coords : add_QD_coords(df)
    #filling missing packets with desired value

    #saving data
    if args.human_output:
        df = fill_missing_packets(df,args.fill_missing)
        print('saving output file (human readable): '+INFO(outfile))
        save_human_readable(df,psds,aux,outfile)
    else:
        #workaround due to patching/updating the code. Do not want to rewrite it
        if args.fill_missing !='None':
            df = fill_missing_packets(df,args.fill_missing)
        print('saving output file (original format): '+INFO(outfile))
        save_original_format(df,aux,info,path+filename,outfile,args.fill_missing)
    msginfo('DONE :)') 

